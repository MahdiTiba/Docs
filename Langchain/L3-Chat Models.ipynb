{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "679fd912-8a5e-4253-9fae-51acbe0657ce",
   "metadata": {},
   "source": [
    "### LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b37fd6-a6e4-4d53-93b4-7320d47c844b",
   "metadata": {},
   "source": [
    "***Large Language Models*** (LLMs) are ***advanced machine learning models*** that excel in a wide range of ***language-related tasks*** without needing task-specific fine tuning for every scenario, such as:\n",
    "- text generation\n",
    "- translation\n",
    "- summarization\n",
    "- question answering\n",
    "- and more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5606fcfa-1753-4460-9870-cf571e17bfe4",
   "metadata": {},
   "source": [
    "### LLM Additional Capabilities\n",
    "- ***Tool calling***: to interact with external services\n",
    "- ***Structured output***:  to make a chat model respond in a structured format, such as JSON\n",
    "- ***Multimodality***: to work with data other than text; for example, images, audio, and video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab0ce00-ca35-42e1-be49-d1c1dc4fe338",
   "metadata": {},
   "source": [
    "### Chat Models\n",
    "In LangChain, the chat model is a way to provide a consistent interface for working with LLMs from different providers while offering additional features for monitoring, debugging, and optimizing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9604be00-3684-4e28-a1fa-ed7fa975cb46",
   "metadata": {},
   "source": [
    "### Initialize Chat Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c42d76-0372-4fc5-968e-8cadff2de871",
   "metadata": {},
   "source": [
    "- ***Install the integration package***\n",
    "\n",
    "`pip install langchain-openai`\n",
    "\n",
    "`pip install langchain-anthropic`\n",
    "\n",
    "You can see a full list of integration packages here [API Reference](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c468d8a-3335-4d8f-b5a2-baccd20f3998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "#pip install python-dotenv\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "321c3226-430c-47c2-abea-86f4518ec605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d375f29-9f87-4c6d-8bc3-900aa614d1ac",
   "metadata": {},
   "source": [
    "### Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96bc8c0a-c9ea-4275-8948-bd038bc8c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_chat_model(\n",
    "    model = 'gpt-4o-mini',\n",
    "    model_provider = 'openai',\n",
    "    temperature = 0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a54407d-0976-413e-8284-1432258fa065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Mahdi! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke('hello, my name is Mahdi')\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2543acb-86db-4551-b6eb-82962af3fac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Type:  <class 'langchain_openai.chat_models.base.ChatOpenAI'>\n",
      "Response Type:  <class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "print('Model Type: ', type(model))\n",
    "print('Response Type: ', type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2b5bc70-8537-4a98-9935-96e174a24e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, Mahdi! How can I assist you today?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b523bf-fec2-4996-9c87-05146970047d",
   "metadata": {},
   "source": [
    "### Calculating Completion Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b170017d-a42b-4573-95d3-f990f2c3929d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 14,\n",
       " 'output_tokens': 12,\n",
       " 'total_tokens': 26,\n",
       " 'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       " 'output_token_details': {'audio': 0, 'reasoning': 0}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0461e37a-1f85-4149-8ea6-332605028b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_pricing = {\n",
    "    'gpt-4o-mini': {\n",
    "        'input': 0.15,\n",
    "        'output': 0.6\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "558d4e96-5dc7-4932-a31e-df916a17e41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Cost: 0.000009$\n"
     ]
    }
   ],
   "source": [
    "input_tokens = response.usage_metadata['input_tokens']\n",
    "output_tokens = response.usage_metadata['output_tokens']\n",
    "\n",
    "cost = (openai_pricing['gpt-4o-mini']['input']*input_tokens + openai_pricing['gpt-4o-mini']['output']*output_tokens) / 1000000\n",
    "print(f\"Total Cost: {cost:8f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ed648-2592-4100-b37f-1e7320921d8e",
   "metadata": {},
   "source": [
    "### Configurable Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe6563ee-2043-4403-96ed-0ebd7b73b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "configurable_model = init_chat_model(temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3929f941-3ed5-4d43-88ef-b29f0e7adc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = configurable_model.invoke(\n",
    "    \"Hello, my name is Mahdi\",\n",
    "    config={\n",
    "        \"configurable\":{\n",
    "            \"model\":'gpt-4o-mini' #\"claude-3-5-sonnet-latest\"\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd57c9de-b79f-45f7-9db7-43733cc778aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello, Mahdi! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 14, 'total_tokens': 26, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CQ88IyAUH36kvj9iYB0vFcWmobGYE', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--f717b8fe-9bb4-4fa9-b64f-09d0dcc8b1c5-0', usage_metadata={'input_tokens': 14, 'output_tokens': 12, 'total_tokens': 26, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f817c99c-198b-45aa-8756-174e4cb53454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41bea4f-6c5e-4699-8874-774909716f17",
   "metadata": {},
   "source": [
    "### Tool Calling\n",
    "\n",
    "- Tool calling allows a chat model to respond to a given prompt by \"calling a tool\".\n",
    "\n",
    "- The model only generates the arguments to a tool, and actually running the tool (or not) is up to the user.\n",
    "\n",
    "![tool_call](images/tool_call.png)\n",
    "\n",
    "- Tool Calling is supported by many popular LLM providers. You can find a list here: [link](https://python.langchain.com/docs/integrations/chat/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "59170eb7-917e-4f4c-992e-9ba715530dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "779bba3c-9711-42b3-b5c2-3c0070bbd084",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetPopulation(BaseModel):\n",
    "    \"\"\"Get the current population in a given location\"\"\"\n",
    "    location: str = Field(..., description=\"The city, e.g. San Francisco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "366a36cb-8497-4d14-80e5-d2f466dad744",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "74288923-ff0f-4c0d-bd77-1aeaf4a134b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools([GetPopulation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f65c13e1-81ea-4dee-9837-df6b3c8a4637",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_with_tools.invoke(\n",
    "    \"What is the population of Madrid?\",\n",
    "    config={\n",
    "        'configurable':{\n",
    "            'model': 'gpt-4o'\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b9c833ef-23f8-4485-ba29-f9ab9bc72c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_F9yzqpLPgbnSR7daFsQRTgD6', 'function': {'arguments': '{\"location\":\"Madrid\"}', 'name': 'GetPopulation'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 63, 'total_tokens': 77, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb3c3cb84d', 'id': 'chatcmpl-CQ8LNTuNSSYm09cyOVOVhUNUMbEEW', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--75d12d58-87a8-49d1-8067-61874607fa5a-0', tool_calls=[{'name': 'GetPopulation', 'args': {'location': 'Madrid'}, 'id': 'call_F9yzqpLPgbnSR7daFsQRTgD6', 'type': 'tool_call'}], usage_metadata={'input_tokens': 63, 'output_tokens': 14, 'total_tokens': 77, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c4e9b8e0-57e9-4cec-a04e-2260426f0e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'GetPopulation',\n",
       "  'args': {'location': 'Madrid'},\n",
       "  'id': 'call_F9yzqpLPgbnSR7daFsQRTgD6',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "350cf9ad-28a8-4663-bc85-8b6ae6ea3ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_with_tools.invoke(\n",
    "    \"What is the population of Madrid in 2025?\",\n",
    "    config={\n",
    "        'configurable':{\n",
    "            'model': 'gpt-4o'\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6cc3bea0-c32d-4fcf-a48e-91a0ed4fbe93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I currently don't have the capability to provide future population estimates, such as the population of Madrid in 2025. For future population projections, you might want to consult demographic studies or reports from relevant statistical agencies or organizations.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 45, 'prompt_tokens': 67, 'total_tokens': 112, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb3c3cb84d', 'id': 'chatcmpl-CQ8LVwIkoxzqAfLC9M8AOwvtb01JF', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--f1add6d8-9131-4646-8d31-b1546456e4ac-0', usage_metadata={'input_tokens': 67, 'output_tokens': 45, 'total_tokens': 112, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "90602912-c2b8-4c9d-ae64-f762808f3532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8a21fe-f359-487d-8fef-edfb46958e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://python.langchain.com/docs/how_to/tool_calling/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
